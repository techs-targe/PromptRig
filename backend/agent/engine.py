"""AI Agent Execution Engine.

The agent can:
- Receive user instructions in natural language
- Use MCP tools to interact with the system via real MCP protocol
- Reason over multiple steps to achieve goals
- Return structured results

MCP Integration:
- Tools are accessed via MCP server (stdio transport)
- MCPClient spawns server subprocess and communicates via JSON-RPC
- Fallback to direct tool registry if MCP is unavailable
"""

import json
import logging
import re
import time
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple
from enum import Enum

from backend.mcp.tools import get_tool_registry, MCPToolRegistry
from backend.mcp.client import MCPClient
from backend.llm.factory import get_llm_client
from backend.database.database import SessionLocal
from backend.database.models import SystemSetting
from backend.agent.policy import (
    PolicyLayer, PolicyDecision, PolicyResult, InputCategory,
    get_policy_layer, wrap_untrusted_content
)
# Legacy intent extractor (rule-based only)
from backend.agent.intent import (
    IntentExtractor, IntentType, Intent, get_intent_extractor
)
# New intent extractor v2 (LLM-based + hierarchical)
from backend.agent.intent_v2 import (
    IntentExtractorV2, IntentV2, Domain, Action, PermissionLevel,
    get_intent_extractor_v2, reset_intent_extractor_v2
)
# Multi-stage LLM guardrail chain
from backend.agent.guardrail_chain import (
    GuardrailChain, GuardrailChainResult, GuardrailDecision, GuardrailStage,
    get_guardrail_chain, reset_guardrail_chain
)
from backend.utils import get_app_name

logger = logging.getLogger(__name__)


class MessageRole(str, Enum):
    """Role of a message in the conversation."""
    SYSTEM = "system"
    USER = "user"
    ASSISTANT = "assistant"
    TOOL = "tool"


@dataclass
class ToolCall:
    """A tool call made by the agent."""
    id: str
    name: str
    arguments: Dict[str, Any]
    result: Optional[Dict[str, Any]] = None


@dataclass
class AgentMessage:
    """A message in the agent conversation."""
    role: MessageRole
    content: str
    tool_calls: List[ToolCall] = field(default_factory=list)
    tool_call_id: Optional[str] = None  # For tool response messages
    timestamp: datetime = field(default_factory=datetime.now)


@dataclass
class PendingConfirmation:
    """Stores info about a tool call awaiting user confirmation."""
    tool_name: str
    arguments: Dict[str, Any]


@dataclass
class AgentSession:
    """Session state for an agent conversation."""
    id: str
    messages: List[AgentMessage] = field(default_factory=list)
    created_at: datetime = field(default_factory=datetime.now)
    model_name: str = "claude-3.5-sonnet"
    temperature: float = 0.7
    max_iterations: int = 10
    current_iteration: int = 0
    status: str = "active"  # active, completed, error, cancelled, terminated
    current_intent: Optional[IntentV2] = None  # Current intent (v2) for the session
    terminated: bool = False  # Session terminated by security guardrail
    pending_confirmation: Optional[PendingConfirmation] = None  # Tool call awaiting confirmation

    def add_message(self, message: AgentMessage):
        """Add a message to the session."""
        self.messages.append(message)

    def get_conversation_history(self) -> List[Dict[str, Any]]:
        """Get conversation history in LLM format."""
        history = []
        for msg in self.messages:
            if msg.role == MessageRole.SYSTEM:
                history.append({"role": "system", "content": msg.content})
            elif msg.role == MessageRole.USER:
                history.append({"role": "user", "content": msg.content})
            elif msg.role == MessageRole.ASSISTANT:
                if msg.tool_calls:
                    # Message with tool calls
                    content = msg.content if msg.content else ""
                    tool_calls_data = []
                    for tc in msg.tool_calls:
                        tool_calls_data.append({
                            "id": tc.id,
                            "type": "function",
                            "function": {
                                "name": tc.name,
                                "arguments": json.dumps(tc.arguments)
                            }
                        })
                    history.append({
                        "role": "assistant",
                        "content": content,
                        "tool_calls": tool_calls_data
                    })
                else:
                    history.append({"role": "assistant", "content": msg.content})
            elif msg.role == MessageRole.TOOL:
                history.append({
                    "role": "tool",
                    "tool_call_id": msg.tool_call_id,
                    "content": msg.content
                })
        return history


class AgentEngine:
    """Engine for running AI agents with tool calling.

    Supports two modes:
    - MCP mode (default): Uses real MCP server via stdio transport
    - Direct mode: Uses in-process tool registry (fallback)
    """

    @staticmethod
    def _get_system_prompt() -> str:
        """Generate system prompt with dynamic app name."""
        app_name = get_app_name()
        # Note: Using string concatenation to avoid f-string brace escaping issues
        # since the prompt contains many literal {{ and }} for template examples
        return "ã‚ãªãŸã¯" + app_name + """ã®é‹ç”¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚
MCPãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç®¡ç†ã—ã¾ã™ã€‚

## âš¡ å¿…é ˆ: ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä½œæˆå‰ã«helpã‚’ç¢ºèª

**ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ä½œæˆãƒ»ç·¨é›†ã™ã‚‹å‰ã«ã€å¿…ãšä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š**

```python
help(topic="workflow")    # ã‚¹ãƒ†ãƒƒãƒ—ã‚¿ã‚¤ãƒ—ã€å¤‰æ•°æ§‹æ–‡ã€æ¼”ç®—å­
help(topic="functions")   # calc, format_choicesç­‰ã®é–¢æ•°
```

ã“ã‚Œã‚’çœç•¥ã™ã‚‹ã¨ã€å¤‰æ•°æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ãƒ»é–¢æ•°ã‚¨ãƒ©ãƒ¼ã§å¤±æ•—ã—ã¾ã™ã€‚

## ðŸš« çµ¶å¯¾éµå®ˆãƒ«ãƒ¼ãƒ«

1. **ã‚¹ãƒ†ãƒƒãƒ—åã¯è‹±èªžã®ã¿** - æ—¥æœ¬èªžã¯100%ã‚¨ãƒ©ãƒ¼
   - âœ… `generate_words`, `check_answer`  âŒ `å•é¡Œæ–‡æ§‹ç¯‰`, `å›žç­”æŠ½å‡º`

2. **condition_config, parser_config ã¯è¾žæ›¸åž‹** - JSONæ–‡å­—åˆ—ã¯ä¸å¯
   - âœ… `condition_config={"assignments": {"x": "0"}}`
   - âŒ `condition_config='{"assignments": {"x": "0"}}'`

3. **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆIDã¯å¿…ãšç¢ºèª** - æŽ¨æ¸¬ç¦æ­¢
   - âœ… `search_datasets("åå‰")` â†’ IDã‚’ç¢ºèªã—ã¦ã‹ã‚‰ä½¿ç”¨

4. **FOREACH/IFãƒ–ãƒ­ãƒƒã‚¯ã¯ãƒšã‚¢ã§é–‰ã˜ã‚‹** - å°‚ç”¨ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨
   - âœ… `add_foreach_block`, `add_if_block` ï¼ˆè‡ªå‹•ã§ãƒšã‚¢ä½œæˆï¼‰

5. **FOREACHå¤‰æ•°ã¯å¿…ãš `{{vars.ROW.column}}` ã§å‚ç…§**
   - âœ… `{{vars.ROW.answerKey}}`  âŒ `{{ROW.answerKey}}`

6. **ã‚«ã‚¦ãƒ³ã‚¿åŠ ç®—ã¯ `calc()` å¿…é ˆ**
   - âœ… `calc({{vars.correct}} + 1)`  âŒ `{{vars.correct}} + 1`

7. **`output` ã‚¹ãƒ†ãƒƒãƒ—ã¯å­˜åœ¨ã—ãªã„** - çµæžœã¯ `set` ã§å¤‰æ•°ã«æ ¼ç´
   - âœ… `step_type="set", condition_config={"assignments": {"result": "..."}}`

8. **choicesç­‰ã®JSONã¯ `format_choices()` ã§æ•´å½¢**
   - âœ… `format_choices({{vars.ROW.choices}})`

9. **ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä½œæˆå¾Œã¯å¿…ãšãƒ†ã‚¹ãƒˆ** - `help(topic="validation")` å‚ç…§
   - âœ… `validate_workflow` (ã‚¨ãƒ©ãƒ¼0ä»¶å¿…é ˆ) â†’ `execute_workflow`
   - âš ï¸ **validate_workflow ãŒæˆåŠŸ(ã‚¨ãƒ©ãƒ¼0ä»¶)ã—ãªã„ã¨ execute_workflow ã¯å®Ÿè¡Œä¸å¯**

## ðŸ”„ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼/ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½œæˆ vs ä¿®æ­£ã®åˆ¤æ–­

**ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œä¿®æ­£ã€ã€Œæ›´æ–°ã€ã€Œå¤‰æ›´ã€ã€Œç›´ã—ã¦ã€ã¨è¨€ã£ãŸå ´åˆ:**

1. **ã¾ãšæ—¢å­˜ãƒªã‚½ãƒ¼ã‚¹ã‚’ç¢ºèª**
   - ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ â†’ `list_workflows()` ã§æ¤œç´¢ã€ã¾ãŸã¯ `get_workflow(id)` ã§ç¢ºèª
   - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ â†’ `list_prompts()` ã§æ¤œç´¢

2. **æ—¢å­˜ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆ â†’ æ›´æ–°ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨**
   - ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä¿®æ­£ â†’ `update_workflow`, `update_workflow_step`, `add_workflow_step`
   - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¿®æ­£ â†’ `update_prompt`

3. **å­˜åœ¨ã—ãªã„å ´åˆã®ã¿ä½œæˆ**
   - `create_workflow`, `create_prompt`

**ã‚ˆãã‚ã‚‹é–“é•ã„:**
- âŒ ã€Œä¿®æ­£ã€ã¨è¨€ã‚ã‚ŒãŸã®ã« `create_workflow` ã‚’å‘¼ã¶ â†’ é‡è¤‡ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãŒç™ºç”Ÿ
- âœ… ã€Œä¿®æ­£ã€ãªã‚‰ `get_workflow` â†’ `update_workflow_step` / `add_workflow_step`

## ðŸ“š MCPãƒ„ãƒ¼ãƒ«ä¸€è¦§ï¼ˆ49å€‹ï¼‰

| ã‚«ãƒ†ã‚´ãƒª | ä¸»è¦ãƒ„ãƒ¼ãƒ« |
|---------|-----------|
| project | list_projects, get_project, create_project, update_project, delete_project, delete_projects |
| prompt | list_prompts, get_prompt, create_prompt, update_prompt, delete_prompt, **clone_prompt** |
| workflow | create_workflow, add_workflow_step, add_foreach_block, add_if_block, remove_workflow_step, get_workflow, list_workflows, validate_workflow, **clone_workflow** |
| execution | execute_prompt, execute_workflow |
| job | get_job_status, list_recent_jobs, download_job_csv |
| dataset | list_datasets, get_dataset, preview_dataset_rows, search_datasets |
| huggingface | import_huggingface_dataset, list_huggingface_datasets, search_huggingface |
| system | list_models, get_system_settings |
| **help** | **help()** ã§ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ«ãƒ¼ãƒ«ã®ãƒ˜ãƒ«ãƒ—ã‚’è¡¨ç¤º |

**ã€Œã€œã‚’å®Ÿè¡Œã—ã¦ã€ã¨è¨€ã‚ã‚ŒãŸã‚‰ `help(topic="execution")` ã‚’ç¢ºèªã—ã¦ã‹ã‚‰å¯¾å¿œ**

## ðŸ’¡ helpãƒ„ãƒ¼ãƒ«ã®ä½¿ã„æ–¹

```python
help()                              # å…¨ãƒ„ãƒ¼ãƒ«ãƒ»ãƒˆãƒ”ãƒƒã‚¯ä¸€è¦§
help(topic="workflow")              # ã‚¹ãƒ†ãƒƒãƒ—ã‚¿ã‚¤ãƒ—ã€å¤‰æ•°ã€æ¼”ç®—å­
help(topic="functions")             # 35å€‹ã®é–¢æ•° (calc, dataset_filter, dataset_joinç­‰)
help(topic="prompt")                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ§‹æ–‡
help(topic="parser")                # ãƒ‘ãƒ¼ã‚µãƒ¼è¨­å®š (json, regexç­‰)
help(topic="dataset_ref")           # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‚ç…§æ§‹æ–‡
help(topic="functions", entry="dataset_filter")  # dataset_filter é–¢æ•°ã®è©³ç´°
```

## ðŸ”§ åŸºæœ¬ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æ§‹ç¯‰æ‰‹é †

1. **help(topic="workflow") ã¨ help(topic="functions") ã‚’ç¢ºèª**
2. `create_workflow` ã§ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä½œæˆ
3. `add_workflow_step` ã§ `set` ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆå¤‰æ•°åˆæœŸåŒ–ï¼‰
4. `add_foreach_block` ã§ãƒ«ãƒ¼ãƒ—é–‹å§‹ï¼ˆè‡ªå‹•ã§ENDFOREACHè¿½åŠ ï¼‰
5. `add_workflow_step` ã§ `prompt` ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆLLMå‘¼ã³å‡ºã—ï¼‰
6. `add_if_block` ã§æ¡ä»¶åˆ†å²ï¼ˆè‡ªå‹•ã§ELSE/ENDIFè¿½åŠ ï¼‰
7. `insert_after` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æŒ¿å…¥ä½ç½®ã‚’æŒ‡å®š
8. æœ€çµ‚çµæžœã¯ `set` ã‚¹ãƒ†ãƒƒãƒ—ã§å¤‰æ•°ã«æ ¼ç´

### é‡è¦ãƒã‚¤ãƒ³ãƒˆ
- `insert_after` ã§æŒ¿å…¥ä½ç½®ã‚’æŒ‡å®šï¼ˆstep_orderã‚ˆã‚Šå®‰å…¨ï¼‰
- `current_structure` ã§ã‚¹ãƒ†ãƒƒãƒ—æ§‹æˆã‚’ç¢ºèª
- **promptã‚¹ãƒ†ãƒƒãƒ—ã«ã¯input_mappingå¿…é ˆ** - ã“ã‚ŒãŒãªã„ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒç©ºã«ãªã‚‹
  - âœ… `input_mapping={"QUESTION": "{{vars.ROW.question}}"}`
- **ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’ä½¿ã†å ´åˆã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§å‡ºåŠ›å½¢å¼ã‚’æŒ‡ç¤º**
  - JSONãƒ‘ãƒ¼ã‚µãƒ¼ â†’ ã€ŒJSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€ã¨æŒ‡ç¤º
  - æ­£è¦è¡¨ç¾ãƒ‘ãƒ¼ã‚µãƒ¼ â†’ æŠ½å‡ºå¯¾è±¡ã®å½¢å¼ã§å‡ºåŠ›ã™ã‚‹ã‚ˆã†æŒ‡ç¤º

## ðŸ”§ æ—¢å­˜ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ä¿®æ­£æ‰‹é †

1. `list_workflows()` ã¾ãŸã¯ `get_workflow(id)` ã§å¯¾è±¡ã‚’ç‰¹å®š
2. æ§‹é€ ã‚’ç¢ºèªã—ã€ä¿®æ­£ãŒå¿…è¦ãªç®‡æ‰€ã‚’ç‰¹å®š
3. é©åˆ‡ãªãƒ„ãƒ¼ãƒ«ã‚’é¸æŠž:

| ä¿®æ­£å†…å®¹ | ä½¿ç”¨ãƒ„ãƒ¼ãƒ« |
|---------|-----------|
| åå‰ãƒ»èª¬æ˜Žå¤‰æ›´ | `update_workflow` |
| ã‚¹ãƒ†ãƒƒãƒ—è¿½åŠ  | `add_workflow_step`, `add_foreach_block`, `add_if_block` |
| ã‚¹ãƒ†ãƒƒãƒ—å¤‰æ›´ | `update_workflow_step` |
| ã‚¹ãƒ†ãƒƒãƒ—å‰Šé™¤ | `delete_workflow_step` |
| ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¿®æ­£ | `update_prompt` (ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå´ã‚’ä¿®æ­£) |

4. `validate_workflow` ã§æ¤œè¨¼

**æ³¨æ„**: æ—¢å­˜ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®æ§‹é€ ã‚’å¤‰æ›´ã™ã‚‹å ´åˆã§ã‚‚ã€`create_workflow` ã¯ä½¿ç”¨ã—ãªã„

## ðŸ“‹ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ vs ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å¤‰æ•°

**é‡è¦**: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å¤‰æ•°ã¯ç•°ãªã‚‹æ§‹æ–‡ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

| ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ | æ§‹æ–‡ | ä¾‹ |
|-------------|------|-----|
| ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ | `{{PARAM}}` | `{{QUESTION}}`, `{{CHOICES}}` |
| ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ (input_mapping, setç­‰) | `{{vars.ROW.xxx}}`, `{{ã‚¹ãƒ†ãƒƒãƒ—å.xxx}}` | `{{vars.ROW.question}}`, `{{ask.ANSWER}}` |

**ã‚ˆãã‚ã‚‹é–“é•ã„**:
- âŒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: `{{vars.ROW.question}}`
- âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: `{{QUESTION}}` + input_mapping: `{"QUESTION": "{{vars.ROW.question}}"}`

**è¿·ã£ãŸã‚‰**: `help(topic="prompt")` ã¨ `help(topic="workflow", entry="input_mapping")` ã‚’ç¢ºèª

## ðŸ”´ ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨å¯¾å‡¦

| ã‚¨ãƒ©ãƒ¼ | åŽŸå›  | å¯¾å‡¦ |
|--------|------|------|
| Unclosed block | endforeach/endifãŒä¸è¶³ | `add_foreach_block`/`add_if_block`ã‚’ä½¿ç”¨ |
| Invalid step_type | æœªçŸ¥ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚¿ã‚¤ãƒ— | `help(topic="workflow")` ã§ç¢ºèª |
| Unknown function | æœªçŸ¥ã®é–¢æ•°å | `help(topic="functions")` ã§ç¢ºèª |
| å¤‰æ•°ãŒç©º | `{{ROW.x}}`ã‚’ä½¿ç”¨ | `{{vars.ROW.x}}`ã«ä¿®æ­£ |
| è¨ˆç®—ã•ã‚Œãªã„ | calc()ã‚’ä½¿ã£ã¦ã„ãªã„ | `calc(å¼)` ã§å›²ã‚€ |
| ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒç©º | input_mappingãŒãªã„ or ã‚­ãƒ¼ä¸ä¸€è‡´ | `help(topic="workflow", entry="input_mapping")` ã‚’å‚ç…§ |
| ãƒ‘ãƒ¼ã‚µãƒ¼ãŒæŠ½å‡ºã§ããªã„ | LLMå‡ºåŠ›ãŒå½¢å¼ã«åˆã‚ãªã„ | ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§å‡ºåŠ›å½¢å¼ã‚’æ˜Žç¤ºçš„ã«æŒ‡ç¤º |
| ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§{{vars.xxx}} | ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ§‹æ–‡ã®èª¤ã‚Š | `help(topic="prompt")`ã§ç¢ºèªã€‚ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¯`{{PARAM}}`ã€å€¤ã¯`input_mapping`ã§æ¸¡ã™ |
| parsed.xxx ãŒ undefined | ãƒ‘ãƒ¼ã‚µãƒ¼ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã®å¤§æ–‡å­—å°æ–‡å­—ä¸ä¸€è‡´ | `help(topic="workflow", entry="case_sensitivity")` ã‚’å‚ç…§ |
| åŒåWFãŒè¤‡æ•°å­˜åœ¨ | ä¿®æ­£æ™‚ã«æ–°è¦ä½œæˆã—ãŸ | `list_workflows`ã§ç¢ºèªã€ä¸è¦ãªã‚‚ã®ã‚’`delete_workflow` |
| ä¿®æ­£ã—ãŸã®ã«åæ˜ ã•ã‚Œãªã„ | åˆ¥ã®WF IDã‚’æ“ä½œã—ãŸ | `get_workflow(id)`ã§ç¢ºèªã—ã¦ã‹ã‚‰ä¿®æ­£ |
| é–¢æ•°ãƒã‚§ãƒ¼ãƒ³ã‚¨ãƒ©ãƒ¼ | `json_parse(x).field`å½¢å¼ã‚’ä½¿ç”¨ | `help(topic="workflow", entry="common_mistakes")` ã‚’å‚ç…§ |

## ðŸ”¤ å¤§æ–‡å­—å°æ–‡å­—ã®åŽ³å¯†ãªãƒ«ãƒ¼ãƒ«ï¼ˆé‡è¦ï¼‰

**å¤§æ–‡å­—å°æ–‡å­—ã¯å¸¸ã«åŽ³å¯†ã«ä¸€è‡´ã•ã›ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ä¸ä¸€è‡´ã¯å‹•ä½œã—ã¾ã›ã‚“ã€‚**

| å®šç¾©å ´æ‰€ | å‚ç…§æ–¹æ³• | ä¾‹ |
|---------|----------|-----|
| ãƒ‘ãƒ¼ã‚µãƒ¼ | ã‚¹ãƒ†ãƒƒãƒ—å‚ç…§ | ãƒ‘ãƒ¼ã‚µãƒ¼: `{"ANSWER": "[A-D]"}` â†’ å‚ç…§: `{{ask.ANSWER}}` (askã¯ã‚¹ãƒ†ãƒƒãƒ—åã€å¤§æ–‡å­—ã§ä¸€è‡´) |
| ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ | input_mapping | ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: `{{QUESTION}}` â†’ input_mapping: `{"QUESTION": "..."}` (å¤§æ–‡å­—ã§ä¸€è‡´) |
| FOREACH | å¤‰æ•°å‚ç…§ | item_var: `"ROW"` â†’ å‚ç…§: `{{vars.ROW.column}}` (ROWã¯å¤§æ–‡å­—å›ºå®š) |

**ã‚ˆãã‚ã‚‹é–“é•ã„:**
- âŒ ãƒ‘ãƒ¼ã‚µãƒ¼ `{"ANSWER": ...}` ã«å¯¾ã—ã¦ `{{ask.answer}}` (å°æ–‡å­—)
- âœ… ãƒ‘ãƒ¼ã‚µãƒ¼ `{"ANSWER": ...}` ã«å¯¾ã—ã¦ `{{ask.ANSWER}}` (å¤§æ–‡å­—ã§ä¸€è‡´ã€askã¯ã‚¹ãƒ†ãƒƒãƒ—å)
- âŒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ `{{QUESTION}}` ã«å¯¾ã—ã¦ `input_mapping: {"question": ...}` (å°æ–‡å­—)
- âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ `{{QUESTION}}` ã«å¯¾ã—ã¦ `input_mapping: {"QUESTION": ...}` (å¤§æ–‡å­—ã§ä¸€è‡´)

## ðŸ” ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¹ãƒ†ãƒƒãƒ—ä½œæˆå‰ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

promptã‚¹ãƒ†ãƒƒãƒ—ã‚’ä½œæˆã™ã‚‹å‰ã«ã€å¿…ãšä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„:

1. **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ç¢ºèª**: `{{PARAM}}` å½¢å¼ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åã‚’ã™ã¹ã¦ç¢ºèª
2. **input_mappingã§ã‚­ãƒ¼ã‚’ä¸€è‡´ã•ã›ã‚‹**: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åã¨å®Œå…¨ä¸€è‡´ï¼ˆå¤§æ–‡å­—å°æ–‡å­—å«ã‚€ï¼‰
3. **ãƒ‘ãƒ¼ã‚µãƒ¼ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã‚’ç¢ºèª**: ãƒ‘ãƒ¼ã‚µãƒ¼ã§å®šç¾©ã—ãŸåå‰ã§å¾Œç¶šã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰å‚ç…§
4. **ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å¤‰æ•°ã‚’ç›´æŽ¥æ›¸ã‹ãªã„**: `{{vars.xxx}}` ã¯input_mappingã§æ¸¡ã™

**ä¾‹:**
```
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: "è³ªå•: {{QUESTION}}\né¸æŠžè‚¢: {{CHOICES}}"
â†“
input_mapping: {"QUESTION": "{{vars.ROW.question}}", "CHOICES": "format_choices({{vars.ROW.choices}})"}
```

## ðŸ“ å¤‰æ•°å‚ç…§æ§‹æ–‡

```
{{input.param}}             - åˆæœŸå…¥åŠ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
{{vars.name}}               - ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å¤‰æ•°
{{ã‚¹ãƒ†ãƒƒãƒ—å.field}}        - ã‚¹ãƒ†ãƒƒãƒ—å‡ºåŠ› (ä¾‹: {{ask.text}})
{{ã‚¹ãƒ†ãƒƒãƒ—å.parsed.FIELD}} - ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸå‡ºåŠ› (ä¾‹: {{ask.ANSWER}})
{{vars.ROW.column}}         - FOREACHã®ç¾åœ¨è¡Œã‚«ãƒ©ãƒ  (varså¿…é ˆ!)
```

**æ³¨æ„**: `step`ã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å®Ÿéš›ã®ã‚¹ãƒ†ãƒƒãƒ—åã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
ä¾‹: ã‚¹ãƒ†ãƒƒãƒ—åãŒ `ask` ãªã‚‰ `{{ask.ANSWER}}`ã€ã‚¹ãƒ†ãƒƒãƒ—åãŒ `generate` ãªã‚‰ `{{generate.text}}`

## âš ï¸ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã®helpå‚ç…§ãƒ«ãƒ¼ãƒ«

ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã‚‰ã€**å¿…ãšå¯¾å¿œã™ã‚‹helpã‚’å‚ç…§**ã—ã¦ã‹ã‚‰ä¿®æ­£ã‚’è©¦ã¿ã¦ãã ã•ã„:

| ã‚¨ãƒ©ãƒ¼ã®ç¨®é¡ž | å‚ç…§ã™ã‚‹help |
|-------------|-------------|
| ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒç©º/ä¸æ­£ | `help(topic="prompt")`, `help(topic="workflow", entry="input_mapping")` |
| å¤‰æ•°æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ | `help(topic="workflow", entry="variables")` |
| é–¢æ•°ã‚¨ãƒ©ãƒ¼ | `help(topic="functions")` |
| ã‚¹ãƒ†ãƒƒãƒ—ã‚¿ã‚¤ãƒ—ã‚¨ãƒ©ãƒ¼ | `help(topic="workflow")` |
| ãƒ‘ãƒ¼ã‚µãƒ¼æŠ½å‡ºå¤±æ•— | `help(topic="parser", entry="prompt_design")` |

**æ‰‹é †**:
1. ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’èª­ã‚€
2. å¯¾å¿œã™ã‚‹helpã‚’å‘¼ã³å‡ºã—ã¦æ­£ã—ã„æ§‹æ–‡ã‚’ç¢ºèª
3. helpã®ä¾‹ã«å¾“ã£ã¦ä¿®æ­£

## ðŸ†˜ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ã€Œhelpå‚ç…§ã€æŒ‡ç¤ºã¸ã®å¯¾å¿œ

ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œhelpã‚’å‚ç…§ã—ã¦ã€ã€Œhelpã‚’è¦‹ã¦ã€ã€Œhelpã§ç¢ºèªã—ã¦ã€ã¨æŒ‡ç¤ºã—ãŸå ´åˆ:

1. **å¿…ãš help() ãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã™** - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã•ãªã„
2. é©åˆ‡ãªãƒˆãƒ”ãƒƒã‚¯ã‚’é¸æŠž:
   - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé–¢é€£ â†’ `help(topic="prompt")`
   - ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é–¢é€£ â†’ `help(topic="workflow")`
   - input_mappingé–¢é€£ â†’ `help(topic="workflow", entry="input_mapping")`
   - é–¢æ•°é–¢é€£ â†’ `help(topic="functions")`
3. helpã®å†…å®¹ã‚’å‚ç…§ã—ã¦å•é¡Œã‚’è§£æ±ºã™ã‚‹

**ä¾‹**:
- ãƒ¦ãƒ¼ã‚¶ãƒ¼: ã€Œinput_mappingãŒç©ºãªã®ã§å€¤ãŒå…¥ã‚‰ãªã„ã€‚helpã‚’å‚ç…§ã—ã¦ã€
- ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ: `help(topic="workflow", entry="input_mapping")` ã‚’å‘¼ã³å‡ºã— â†’ çµæžœã‚’åŸºã«ä¿®æ­£ææ¡ˆ

## âš ï¸ ãã®ä»–ã®æ³¨æ„äº‹é …

- **ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä½œæˆå‰ã« help(topic="workflow") ã¨ help(topic="functions") ã‚’å¿…ãšç¢ºèª**
- URLã¯ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§å‡ºåŠ›ï¼ˆMarkdownå½¢å¼ç¦æ­¢ï¼‰

ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: backend/agent/system_prompt_backup.py ã«æ—§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¿å­˜æ¸ˆã¿"""

    def __init__(
        self,
        model_name: str = None,
        temperature: float = 0.7,
        use_mcp: bool = True,
        use_intent_v2: bool = True,
        intent_classifier_model: str = "openai-gpt-4.1-nano",
        use_guardrail_chain: bool = True,
        guardrail_model: str = None,
    ):
        """Initialize the agent engine.

        Args:
            model_name: LLM model to use for agent responses
            temperature: Temperature for LLM
            use_mcp: Whether to use real MCP server (True) or direct tool registry (False)
            use_intent_v2: Whether to use new LLM-based intent classification (True)
                          or legacy rule-based only (False)
            intent_classifier_model: Model to use for intent classification
                                    (e.g., 'openai-gpt-4.1-nano', 'azure-gpt-5-nano')
            use_guardrail_chain: Whether to use multi-stage LLM guardrail chain
            guardrail_model: Model for guardrail checks (uses system setting if None)
        """
        self.use_mcp = use_mcp
        self.use_intent_v2 = use_intent_v2
        self.use_guardrail_chain = use_guardrail_chain
        self.tool_registry = get_tool_registry()  # Keep for fallback and tool schema
        self.mcp_client: Optional[MCPClient] = MCPClient() if use_mcp else None
        self.model_name = model_name or self._get_default_model()
        self.temperature = temperature
        self.sessions: Dict[str, AgentSession] = {}
        self.policy_layer = get_policy_layer()  # Security policy layer
        self.event_callback: Optional[callable] = None  # Callback for real-time event streaming

        # Multi-stage LLM guardrail chain
        if use_guardrail_chain:
            self.guardrail_chain = get_guardrail_chain(guardrail_model)
            logger.info(f"[Agent] Using guardrail chain with model: {self.guardrail_chain.model_name}")
        else:
            self.guardrail_chain = None
            logger.info("[Agent] Guardrail chain disabled")

        # Intent extraction layer (v2 = LLM-based + hierarchical, v1 = rule-based only)
        if use_intent_v2:
            self.intent_extractor_v2 = get_intent_extractor_v2(
                classifier_model=intent_classifier_model,
                use_llm=True,
            )
            self.intent_extractor = None  # Not used in v2 mode
            logger.info(f"[Agent] Using intent v2 with classifier: {intent_classifier_model}")
        else:
            self.intent_extractor = get_intent_extractor()  # Legacy
            self.intent_extractor_v2 = None
            logger.info("[Agent] Using legacy intent extractor (rule-based only)")

    def _get_default_model(self) -> str:
        """Get default model from system settings."""
        db = SessionLocal()
        try:
            setting = db.query(SystemSetting).filter(
                SystemSetting.key == "active_llm_model"
            ).first()
            return setting.value if setting else "claude-3.5-sonnet"
        finally:
            db.close()

    def _get_max_iterations(self) -> int:
        """Get max iterations from system settings."""
        db = SessionLocal()
        try:
            setting = db.query(SystemSetting).filter(
                SystemSetting.key == "agent_max_iterations"
            ).first()
            if setting and setting.value:
                try:
                    max_iter = int(setting.value)
                    # Clamp to valid range (10-99)
                    return max(10, min(max_iter, 99))
                except ValueError:
                    pass
            return 30  # Default value
        finally:
            db.close()

    def _get_max_tokens(self) -> int:
        """Get max completion tokens from system settings.

        Reasoning models (GPT-5, o4-mini) use many tokens for internal
        thinking before generating output. If agents return empty responses
        with finish_reason=length, this value needs to be increased.
        """
        db = SessionLocal()
        try:
            setting = db.query(SystemSetting).filter(
                SystemSetting.key == "agent_max_tokens"
            ).first()
            if setting and setting.value:
                try:
                    max_tokens = int(setting.value)
                    # Clamp to valid range (1024-65536)
                    return max(1024, min(max_tokens, 65536))
                except ValueError:
                    pass
            return 16384  # Default value for reasoning models
        finally:
            db.close()

    def _get_llm_timeout(self) -> float:
        """Get LLM API timeout from system settings.

        Controls how long to wait for OpenAI API responses before timing out.
        Default is 600 seconds (10 minutes) to handle slow API responses.
        """
        db = SessionLocal()
        try:
            setting = db.query(SystemSetting).filter(
                SystemSetting.key == "agent_llm_timeout"
            ).first()
            if setting and setting.value:
                try:
                    timeout = int(setting.value)
                    # Clamp to valid range (60-1800 seconds = 1-30 minutes)
                    return float(max(60, min(timeout, 1800)))
                except ValueError:
                    pass
            return 600.0  # Default: 10 minutes
        finally:
            db.close()

    def create_session(self, session_id: str = None,
                       model_name: str = None,
                       temperature: float = None,
                       max_iterations: int = None) -> AgentSession:
        """Create a new agent session.

        Args:
            session_id: Session identifier (auto-generated if not provided)
            model_name: LLM model to use
            temperature: LLM temperature
            max_iterations: Maximum iterations (overrides system setting if provided)
        """
        if session_id is None:
            session_id = f"agent_{int(time.time() * 1000)}"

        # Get max_iterations from parameter or system settings
        if max_iterations is None:
            max_iterations = self._get_max_iterations()
        else:
            # Clamp to valid range
            max_iterations = max(10, min(max_iterations, 99))

        session = AgentSession(
            id=session_id,
            model_name=model_name or self.model_name,
            temperature=temperature if temperature is not None else self.temperature,
            max_iterations=max_iterations
        )

        # Add system prompt
        session.add_message(AgentMessage(
            role=MessageRole.SYSTEM,
            content=self._get_system_prompt()
        ))

        self.sessions[session_id] = session
        return session

    def get_session(self, session_id: str) -> Optional[AgentSession]:
        """Get an existing session."""
        return self.sessions.get(session_id)

    def _emit_event(self, event_type: str, message: str, data: Optional[Dict] = None) -> None:
        """Emit an event to the callback if registered."""
        if self.event_callback:
            try:
                self.event_callback(event_type, message, data)
            except Exception as e:
                logger.warning(f"Event callback error: {e}")

    async def run(self, session: AgentSession, user_message: str) -> str:
        """Run the agent with a user message.

        Args:
            session: The agent session
            user_message: The user's message

        Returns:
            The agent's final response
        """
        # Check if session is terminated (security guardrail)
        if session.terminated:
            return "ã“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¯ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ä¸Šã®ç†ç”±ã«ã‚ˆã‚Šçµ‚äº†ã—ã¦ã„ã¾ã™ã€‚æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚"

        # MULTI-STAGE LLM GUARDRAIL CHAIN (runs BEFORE intent extraction)
        if self.use_guardrail_chain and self.guardrail_chain:
            # Build conversation history for security check
            conversation_history = self._get_recent_conversation_text(session, limit=5)

            guardrail_result = self.guardrail_chain.check(
                user_message=user_message,
                conversation_history=conversation_history
            )

            logger.info(
                f"[Guardrail] Session {session.id}: passed={guardrail_result.passed}, "
                f"latency={guardrail_result.total_latency_ms}ms"
            )

            if not guardrail_result.passed:
                # Guardrail rejected the request
                rejection_message = guardrail_result.rejection_message

                # Log stage results for debugging
                for stage_result in guardrail_result.stage_results:
                    logger.info(
                        f"[Guardrail] Stage {stage_result.stage.value}: "
                        f"{stage_result.decision.value} - {stage_result.reason}"
                    )

                # Handle session termination (security threat)
                if guardrail_result.terminate_session:
                    session.terminated = True
                    session.status = "terminated"
                    logger.warning(f"[Guardrail] Session {session.id} TERMINATED due to security threat")

                # Add messages to session
                session.add_message(AgentMessage(
                    role=MessageRole.USER,
                    content=user_message
                ))
                session.add_message(AgentMessage(
                    role=MessageRole.ASSISTANT,
                    content=rejection_message
                ))

                return rejection_message

        # Use intent v2 (LLM-based) or legacy (rule-based only)
        if self.use_intent_v2:
            return await self._run_with_intent_v2(session, user_message)
        else:
            return await self._run_with_legacy_intent(session, user_message)

    def _get_recent_conversation_text(self, session: AgentSession, limit: int = 5) -> str:
        """Get recent conversation as text for security check."""
        recent_messages = []
        count = 0
        for msg in reversed(session.messages):
            if msg.role in (MessageRole.USER, MessageRole.ASSISTANT):
                role_label = "User" if msg.role == MessageRole.USER else "Assistant"
                recent_messages.append(f"{role_label}: {msg.content[:500]}")
                count += 1
                if count >= limit:
                    break
        recent_messages.reverse()
        return "\n".join(recent_messages) if recent_messages else "(ä¼šè©±å±¥æ­´ãªã—)"

    async def _run_with_intent_v2(self, session: AgentSession, user_message: str) -> str:
        """Run agent with new LLM-based intent classification (v2).

        Intent v2 handles security filtering internally with SecurityPreFilter.
        """
        # Build conversation history for context-aware intent classification
        conversation_history = self._get_recent_conversation_text(session, limit=5)

        # INTENT EXTRACTION v2: Security + LLM-based classification + Rule fallback
        intent = self.intent_extractor_v2.extract(user_message, conversation_history)

        logger.info(
            f"[Intent-v2] Session {session.id}: "
            f"domain={intent.domain.value}, action={intent.action.value}, "
            f"confidence={intent.confidence:.2f}, method={intent.classification_method}, "
            f"permission={intent.permission_level.value}"
        )

        # Check if intent is allowed
        if not intent.is_allowed():
            # Out-of-scope or security threat - reject immediately
            rejection_response = self.intent_extractor_v2.get_rejection_message(intent)
            logger.info(f"[Intent-v2] Rejected request in session {session.id}: {intent.domain.value}")
            session.add_message(AgentMessage(
                role=MessageRole.USER,
                content=user_message
            ))
            session.add_message(AgentMessage(
                role=MessageRole.ASSISTANT,
                content=rejection_response
            ))
            return rejection_response

        # HELP intent: LLMã«å‡¦ç†ã•ã›ã¦help MCPãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã›ã‚‹ã‚ˆã†ã«ã™ã‚‹
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œhelpã‚’å‚ç…§ã—ã¦ã€ã¨è¨€ã£ãŸå ´åˆã€help() ãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã™ãŸã‚ã«
        # é€šå¸¸ã®LLMå‡¦ç†ãƒ•ãƒ­ãƒ¼ã«ç§»è¡Œã•ã›ã‚‹ï¼ˆç‰¹åˆ¥æ‰±ã„ã‚’å‰Šé™¤ï¼‰
        # Note: ç´”ç²‹ãªhelpè³ªå•ï¼ˆã€Œä½•ãŒã§ãã‚‹ï¼Ÿã€ãªã©ï¼‰ã‚‚LLMãŒé©åˆ‡ã«å‡¦ç†ã™ã‚‹

        # Handle CONFIRM action - auto-execute confirmed tool immediately
        if intent.action == Action.CONFIRM:
            logger.info(f"[Intent-v2] Confirmation detected in session {session.id}: {intent.parameters}")
            pending = self.policy_layer.get_pending_confirmation(session.id)

            if pending and intent.parameters.get("confirmed", True):
                tool_name, arguments = pending

                # Clear pending confirmation first
                self.policy_layer.clear_pending_confirmation(session.id)

                # Add user message
                session.add_message(AgentMessage(
                    role=MessageRole.USER,
                    content=user_message
                ))

                # AUTO-EXECUTE the confirmed tool immediately (bypass LLM loop)
                logger.info(f"[Intent-v2] Auto-executing confirmed tool: {tool_name}")
                self._emit_event("tool_start", f"ç¢ºèªæ¸ˆã¿ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œä¸­: {tool_name}", {
                    "tool_name": tool_name,
                    "arguments": arguments
                })

                try:
                    if self.use_mcp and self.mcp_client:
                        # Establish MCP connection for confirmed tool execution
                        async with self.mcp_client.connect():
                            result = await self.mcp_client.call_tool(tool_name, arguments)
                    else:
                        tool_context = {"default_model": session.model_name}
                        result = await self.tool_registry.execute_tool(
                            tool_name, arguments, context=tool_context
                        )

                    self._emit_event("tool_end", f"ãƒ„ãƒ¼ãƒ«å®Œäº†: {tool_name}", {
                        "tool_name": tool_name,
                        "success": result.get("success", False) if isinstance(result, dict) else True
                    })

                    # Format and return result
                    if isinstance(result, dict) and result.get("success"):
                        response = f"å®Ÿè¡Œå®Œäº†: {tool_name}\nçµæžœ: {json.dumps(result.get('result', result), ensure_ascii=False, indent=2)}"
                    else:
                        error = result.get("error", "Unknown error") if isinstance(result, dict) else str(result)
                        response = f"å®Ÿè¡Œå¤±æ•—: {tool_name}\nã‚¨ãƒ©ãƒ¼: {error}"

                except Exception as e:
                    logger.error(f"[Intent-v2] Error executing confirmed tool: {e}", exc_info=True)
                    response = f"å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {tool_name}\nã‚¨ãƒ©ãƒ¼: {str(e)}"

                session.add_message(AgentMessage(
                    role=MessageRole.ASSISTANT,
                    content=response
                ))
                session.status = "completed"
                return response

            elif pending and not intent.parameters.get("confirmed", True):
                # User said "no" - clear pending and inform
                self.policy_layer.clear_pending_confirmation(session.id)
                session.add_message(AgentMessage(
                    role=MessageRole.USER,
                    content=user_message
                ))
                response = "æ“ä½œã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸã€‚"
                session.add_message(AgentMessage(
                    role=MessageRole.ASSISTANT,
                    content=response
                ))
                return response

        # Add user message
        session.add_message(AgentMessage(
            role=MessageRole.USER,
            content=user_message
        ))

        # Store intent in session for potential use by tools
        session.current_intent = intent

        # Get LLM client
        client = get_llm_client(session.model_name)

        # Run with MCP or direct mode
        if self.use_mcp and self.mcp_client:
            response = await self._run_with_mcp(session, client)
        else:
            response = await self._run_direct(session, client)

        # Apply output filter to prevent system prompt leakage
        filtered_response, was_filtered = self.policy_layer.filter_output(response, user_message)
        if was_filtered:
            logger.warning(f"Output filtered for session {session.id}")
            if session.messages and session.messages[-1].role == MessageRole.ASSISTANT:
                session.messages[-1].content = filtered_response

        return filtered_response

    async def _run_with_legacy_intent(self, session: AgentSession, user_message: str) -> str:
        """Run agent with legacy rule-based intent classification (v1)."""
        # SECURITY: Input filtering at the entry point (BEFORE LLM sees anything)
        # This is the PRIMARY security gate - if blocked here, LLM never sees the request
        should_block, rejection_msg, category = self.policy_layer.filter_user_input(user_message)

        if should_block:
            logger.warning(f"[Security] Blocked input in session {session.id}, category: {category.value}")
            session.add_message(AgentMessage(
                role=MessageRole.USER,
                content=user_message
            ))
            session.add_message(AgentMessage(
                role=MessageRole.ASSISTANT,
                content=rejection_msg
            ))
            return rejection_msg

        # INTENT EXTRACTION: Determine if request is within scope (WITHOUT LLM)
        # Only system operation requests are allowed; all others are rejected immediately
        intent = self.intent_extractor.extract(user_message)
        logger.info(f"[Intent] Session {session.id}: type={intent.intent_type.value}, "
                   f"confidence={intent.confidence:.2f}, keywords={intent.matched_keywords}")

        if not self.intent_extractor.is_allowed(intent):
            # Out-of-scope request - reject immediately without LLM
            rejection_response = self.intent_extractor.get_rejection_message(intent)
            logger.info(f"[Intent] Rejected out-of-scope request in session {session.id}")
            session.add_message(AgentMessage(
                role=MessageRole.USER,
                content=user_message
            ))
            session.add_message(AgentMessage(
                role=MessageRole.ASSISTANT,
                content=rejection_response
            ))
            return rejection_response

        # Handle HELP intent directly (no LLM needed)
        if intent.intent_type == IntentType.HELP:
            help_response = self.intent_extractor.get_help_message()
            logger.info(f"[Intent] Providing help response in session {session.id}")
            session.add_message(AgentMessage(
                role=MessageRole.USER,
                content=user_message
            ))
            session.add_message(AgentMessage(
                role=MessageRole.ASSISTANT,
                content=help_response
            ))
            return help_response

        # Add user message
        session.add_message(AgentMessage(
            role=MessageRole.USER,
            content=user_message
        ))

        # Get LLM client
        client = get_llm_client(session.model_name)

        # Run with MCP or direct mode
        if self.use_mcp and self.mcp_client:
            response = await self._run_with_mcp(session, client)
        else:
            response = await self._run_direct(session, client)

        # Apply output filter to prevent system prompt leakage
        filtered_response, was_filtered = self.policy_layer.filter_output(response, user_message)
        if was_filtered:
            logger.warning(f"Output filtered for session {session.id}")
            # Update the last assistant message with filtered content
            if session.messages and session.messages[-1].role == MessageRole.ASSISTANT:
                session.messages[-1].content = filtered_response

        return filtered_response

    async def _run_with_mcp(self, session: AgentSession, client) -> str:
        """Run agent loop using real MCP server.

        Tools are executed via MCP protocol (stdio transport, JSON-RPC).
        """
        logger.info("Running agent in MCP mode")

        async with self.mcp_client.connect():
            # Set default model for execute_workflow/execute_prompt
            if session.model_name:
                logger.info(f"[MCP] Setting default model to: {session.model_name}")
                await self.mcp_client.call_tool("set_default_model", {"model_name": session.model_name})

            # Get tool schemas from MCP server
            mcp_tools = await self.mcp_client.list_tools()

            # Convert to OpenAI function calling format
            tools = []
            for tool in mcp_tools:
                tools.append({
                    "type": "function",
                    "function": {
                        "name": tool["name"],
                        "description": tool["description"],
                        "parameters": tool["inputSchema"]
                    }
                })

            # Run agent loop
            while session.current_iteration < session.max_iterations:
                session.current_iteration += 1
                self._emit_event("iteration", f"ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ {session.current_iteration}/{session.max_iterations}", {
                    "current": session.current_iteration,
                    "max": session.max_iterations
                })

                # Build messages for LLM
                messages = session.get_conversation_history()

                # Call LLM with tools
                self._emit_event("llm_call", "LLMã«å•ã„åˆã‚ã›ä¸­...")
                response = await self._call_llm_with_tools(
                    client, messages, tools, session.temperature
                )

                if response is None:
                    session.status = "error"
                    self._emit_event("error", "LLMã‹ã‚‰ã®å¿œç­”å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                    return "Error: Failed to get response from LLM"

                # Check if we have tool calls
                if response.get("tool_calls"):
                    # Process tool calls
                    assistant_msg = AgentMessage(
                        role=MessageRole.ASSISTANT,
                        content=response.get("content", ""),
                        tool_calls=[]
                    )

                    for tc in response["tool_calls"]:
                        tool_call = ToolCall(
                            id=tc["id"],
                            name=tc["function"]["name"],
                            arguments=json.loads(tc["function"]["arguments"])
                        )

                        # Policy Layer evaluation before execution
                        policy_result = self.policy_layer.evaluate(
                            tool_call.name,
                            tool_call.arguments,
                            session.id
                        )

                        if policy_result.decision == PolicyDecision.DENY:
                            # Tool execution denied by policy
                            logger.warning(f"[Policy] DENIED tool: {tool_call.name} - {policy_result.reason}")
                            tool_call.result = {
                                "success": False,
                                "error": f"Policy denied: {policy_result.reason}",
                                "policy_decision": "denied"
                            }
                        elif policy_result.decision == PolicyDecision.NEEDS_CONFIRMATION:
                            # Tool requires user confirmation - return confirmation request
                            logger.info(f"[Policy] Tool {tool_call.name} needs user confirmation")
                            # Store pending confirmation in policy layer (persists across requests)
                            self.policy_layer.set_pending_confirmation(
                                session.id, tool_call.name, tool_call.arguments
                            )
                            confirmation_prompt = self.policy_layer.get_confirmation_prompt(
                                tool_call.name, tool_call.arguments
                            )
                            tool_call.result = {
                                "success": False,
                                "error": "User confirmation required",
                                "policy_decision": "needs_confirmation",
                                "confirmation_prompt": confirmation_prompt
                            }
                            # Add tool call to message history before returning
                            assistant_msg.tool_calls.append(tool_call)
                            session.add_message(assistant_msg)
                            session.add_message(AgentMessage(
                                role=MessageRole.TOOL,
                                content=json.dumps(tool_call.result, ensure_ascii=False),
                                tool_call_id=tool_call.id
                            ))
                            # Exit loop and return confirmation prompt to user
                            session.status = "waiting_confirmation"
                            return confirmation_prompt
                        else:
                            # Tool execution allowed
                            logger.info(f"[MCP] Executing tool: {tool_call.name} with args: {tool_call.arguments}")
                            self._emit_event("tool_start", f"ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œä¸­: {tool_call.name}", {
                                "tool_name": tool_call.name,
                                "arguments": tool_call.arguments
                            })
                            result = await self.mcp_client.call_tool(
                                tool_call.name,
                                tool_call.arguments
                            )
                            # Wrap tool output as untrusted content for LLM
                            tool_call.result = result
                            self._emit_event("tool_end", f"ãƒ„ãƒ¼ãƒ«å®Œäº†: {tool_call.name}", {
                                "tool_name": tool_call.name,
                                "success": result.get("success", False) if isinstance(result, dict) else True
                            })

                        assistant_msg.tool_calls.append(tool_call)

                    session.add_message(assistant_msg)

                    # Add tool results as messages
                    for tc in assistant_msg.tool_calls:
                        session.add_message(AgentMessage(
                            role=MessageRole.TOOL,
                            content=json.dumps(tc.result, ensure_ascii=False),
                            tool_call_id=tc.id
                        ))

                else:
                    # No tool calls - final response
                    final_content = response.get("content", "")
                    self._emit_event("llm_response", "LLMã‹ã‚‰ã®å¿œç­”ã‚’å—ä¿¡ã—ã¾ã—ãŸ", {
                        "response_length": len(final_content)
                    })
                    session.add_message(AgentMessage(
                        role=MessageRole.ASSISTANT,
                        content=final_content
                    ))
                    session.status = "completed"
                    return final_content

            # Max iterations reached
            session.status = "completed"
            self._emit_event("max_iterations", "æœ€å¤§ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°ã«é”ã—ã¾ã—ãŸ")
            return "I've reached the maximum number of steps. Please let me know if you'd like me to continue."

    async def _run_direct(self, session: AgentSession, client) -> str:
        """Run agent loop using direct tool registry (fallback mode).

        Tools are executed directly in-process without MCP protocol.
        """
        logger.info("Running agent in direct mode (fallback)")

        # Run agent loop
        while session.current_iteration < session.max_iterations:
            session.current_iteration += 1
            self._emit_event("iteration", f"ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ {session.current_iteration}/{session.max_iterations}", {
                "current": session.current_iteration,
                "max": session.max_iterations
            })

            # Build messages for LLM
            messages = session.get_conversation_history()

            # Get tool schemas from local registry
            tools = self.tool_registry.get_tools_json_schema()

            # Call LLM with tools
            self._emit_event("llm_call", "LLMã«å•ã„åˆã‚ã›ä¸­...")
            response = await self._call_llm_with_tools(
                client, messages, tools, session.temperature
            )

            if response is None:
                session.status = "error"
                self._emit_event("error", "LLMã‹ã‚‰ã®å¿œç­”å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return "Error: Failed to get response from LLM"

            # Check if we have tool calls
            if response.get("tool_calls"):
                # Process tool calls
                assistant_msg = AgentMessage(
                    role=MessageRole.ASSISTANT,
                    content=response.get("content", ""),
                    tool_calls=[]
                )

                for tc in response["tool_calls"]:
                    tool_call = ToolCall(
                        id=tc["id"],
                        name=tc["function"]["name"],
                        arguments=json.loads(tc["function"]["arguments"])
                    )

                    # Policy Layer evaluation before execution
                    policy_result = self.policy_layer.evaluate(
                        tool_call.name,
                        tool_call.arguments,
                        session.id
                    )

                    if policy_result.decision == PolicyDecision.DENY:
                        # Tool execution denied by policy
                        logger.warning(f"[Policy] DENIED tool: {tool_call.name} - {policy_result.reason}")
                        tool_call.result = {
                            "success": False,
                            "error": f"Policy denied: {policy_result.reason}",
                            "policy_decision": "denied"
                        }
                    elif policy_result.decision == PolicyDecision.NEEDS_CONFIRMATION:
                        # Tool requires user confirmation
                        logger.info(f"[Policy] Tool {tool_call.name} needs user confirmation")
                        # Store pending confirmation in policy layer (persists across requests)
                        self.policy_layer.set_pending_confirmation(
                            session.id, tool_call.name, tool_call.arguments
                        )
                        confirmation_prompt = self.policy_layer.get_confirmation_prompt(
                            tool_call.name, tool_call.arguments
                        )
                        tool_call.result = {
                            "success": False,
                            "error": "User confirmation required",
                            "policy_decision": "needs_confirmation",
                            "confirmation_prompt": confirmation_prompt
                        }
                        # Add tool call to message history before returning
                        assistant_msg.tool_calls.append(tool_call)
                        session.add_message(assistant_msg)
                        session.add_message(AgentMessage(
                            role=MessageRole.TOOL,
                            content=json.dumps(tool_call.result, ensure_ascii=False),
                            tool_call_id=tool_call.id
                        ))
                        # Exit loop and return confirmation prompt to user
                        session.status = "waiting_confirmation"
                        return confirmation_prompt
                    else:
                        # Tool execution allowed
                        logger.info(f"[Direct] Executing tool: {tool_call.name} with args: {tool_call.arguments}")
                        self._emit_event("tool_start", f"ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œä¸­: {tool_call.name}", {
                            "tool_name": tool_call.name,
                            "arguments": tool_call.arguments
                        })
                        # Pass session model as context so tools use the correct model
                        tool_context = {"default_model": session.model_name}
                        result = await self.tool_registry.execute_tool(
                            tool_call.name,
                            tool_call.arguments,
                            context=tool_context
                        )
                        tool_call.result = result
                        self._emit_event("tool_end", f"ãƒ„ãƒ¼ãƒ«å®Œäº†: {tool_call.name}", {
                            "tool_name": tool_call.name,
                            "success": result.get("success", False) if isinstance(result, dict) else True
                        })

                    assistant_msg.tool_calls.append(tool_call)

                session.add_message(assistant_msg)

                # Add tool results as messages
                for tc in assistant_msg.tool_calls:
                    session.add_message(AgentMessage(
                        role=MessageRole.TOOL,
                        content=json.dumps(tc.result, ensure_ascii=False),
                        tool_call_id=tc.id
                    ))

            else:
                # No tool calls - final response
                final_content = response.get("content", "")
                self._emit_event("llm_response", "LLMã‹ã‚‰ã®å¿œç­”ã‚’å—ä¿¡ã—ã¾ã—ãŸ", {
                    "response_length": len(final_content)
                })
                session.add_message(AgentMessage(
                    role=MessageRole.ASSISTANT,
                    content=final_content
                ))
                session.status = "completed"
                return final_content

        # Max iterations reached
        session.status = "completed"
        self._emit_event("max_iterations", "æœ€å¤§ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°ã«é”ã—ã¾ã—ãŸ")
        return "I've reached the maximum number of steps. Please let me know if you'd like me to continue."

    async def _call_llm_with_tools(self, client, messages: List[Dict],
                                   tools: List[Dict], temperature: float) -> Optional[Dict]:
        """Call LLM with tool support.

        This method handles the tool calling protocol for different LLM providers.
        """
        try:
            # For Claude models, we need to format differently
            if "claude" in client.__class__.__name__.lower() or "anthropic" in client.__class__.__name__.lower():
                return await self._call_claude_with_tools(client, messages, tools, temperature)
            else:
                # For OpenAI-compatible models
                return await self._call_openai_with_tools(client, messages, tools, temperature)
        except Exception as e:
            logger.error(f"Error calling LLM: {e}", exc_info=True)
            return None

    async def _call_claude_with_tools(self, client, messages: List[Dict],
                                      tools: List[Dict], temperature: float) -> Dict:
        """Call Claude with native tool use.

        Uses the passed client's anthropic SDK instance and model name.
        """
        # Convert tools to Claude format
        claude_tools = []
        for tool in tools:
            func = tool["function"]
            claude_tools.append({
                "name": func["name"],
                "description": func["description"],
                "input_schema": func["parameters"]
            })

        # Separate system message
        system_content = ""
        api_messages = []
        for msg in messages:
            if msg["role"] == "system":
                system_content = msg["content"]
            elif msg["role"] == "tool":
                # Convert tool response to Claude format
                api_messages.append({
                    "role": "user",
                    "content": [{
                        "type": "tool_result",
                        "tool_use_id": msg.get("tool_call_id", ""),
                        "content": msg["content"]
                    }]
                })
            elif msg["role"] == "assistant" and msg.get("tool_calls"):
                # Convert tool calls to Claude format
                content = []
                if msg.get("content"):
                    content.append({"type": "text", "text": msg["content"]})
                for tc in msg["tool_calls"]:
                    content.append({
                        "type": "tool_use",
                        "id": tc["id"],
                        "name": tc["function"]["name"],
                        "input": json.loads(tc["function"]["arguments"])
                    })
                api_messages.append({"role": "assistant", "content": content})
            else:
                api_messages.append(msg)

        # Use the client's anthropic instance and model name
        # The client is an AnthropicClaudeClient with a 'client' attribute (anthropic.Anthropic)
        # and a MODEL_NAME attribute
        # Get max tokens from system settings
        max_tokens = self._get_max_tokens()
        response = client.client.messages.create(
            model=client.MODEL_NAME,
            max_tokens=max_tokens,
            system=system_content,
            messages=api_messages,
            tools=claude_tools,
            temperature=temperature
        )

        # Parse response
        result = {"content": "", "tool_calls": []}

        for block in response.content:
            if block.type == "text":
                result["content"] += block.text
            elif block.type == "tool_use":
                result["tool_calls"].append({
                    "id": block.id,
                    "type": "function",
                    "function": {
                        "name": block.name,
                        "arguments": json.dumps(block.input)
                    }
                })

        return result

    async def _call_openai_with_tools(self, client, messages: List[Dict],
                                       tools: List[Dict], temperature: float) -> Dict:
        """Call OpenAI-compatible API with native tool calling."""
        # Use native OpenAI tool calling API
        # The client has a .client attribute which is the OpenAI SDK client

        try:
            # Build API messages - convert our format to OpenAI format
            api_messages = []
            for msg in messages:
                if msg["role"] == "system":
                    api_messages.append({"role": "system", "content": msg["content"]})
                elif msg["role"] == "user":
                    api_messages.append({"role": "user", "content": msg["content"]})
                elif msg["role"] == "assistant":
                    if msg.get("tool_calls"):
                        # Assistant message with tool calls
                        api_msg = {
                            "role": "assistant",
                            "content": msg.get("content") or None,
                            "tool_calls": [
                                {
                                    "id": tc["id"],
                                    "type": "function",
                                    "function": {
                                        "name": tc["function"]["name"],
                                        "arguments": tc["function"]["arguments"]
                                    }
                                }
                                for tc in msg["tool_calls"]
                            ]
                        }
                        api_messages.append(api_msg)
                    else:
                        api_messages.append({"role": "assistant", "content": msg.get("content", "")})
                elif msg["role"] == "tool":
                    api_messages.append({
                        "role": "tool",
                        "tool_call_id": msg["tool_call_id"],
                        "content": msg["content"]
                    })

            # Get model name from client
            model_name = getattr(client, 'MODEL_NAME', 'gpt-4o-mini')

            # Check if this is a GPT-5 or o4 model (fixed temperature, uses max_completion_tokens)
            is_fixed_temp_model = "gpt-5" in model_name or "o4-mini" in model_name or "o4" in model_name

            # Build API call parameters
            call_params = {
                "model": model_name,
                "messages": api_messages,
                "tools": tools,
            }

            # GPT-5/o4 models: fixed temperature=1, use max_completion_tokens
            # Other models: configurable temperature, use max_tokens
            # Note: Reasoning models (o4, gpt-5) need higher token limits because they
            # use many tokens for internal reasoning before generating output.
            # If finish_reason=length with 0 content, increase this value.
            # Get max tokens from system settings
            max_tokens = self._get_max_tokens()

            if is_fixed_temp_model:
                call_params["max_completion_tokens"] = max_tokens
                # Don't pass temperature - GPT-5/o4 only supports default (1.0)
            else:
                call_params["temperature"] = temperature
                call_params["max_tokens"] = max_tokens

            # Get LLM timeout from system settings
            llm_timeout = self._get_llm_timeout()

            # Call OpenAI API with native tool calling and timeout
            logger.info(f"[Agent] Calling OpenAI API with model={call_params.get('model')}, messages={len(call_params.get('messages', []))}, tools={len(call_params.get('tools', []))}, timeout={llm_timeout}s")
            response = client.client.chat.completions.create(**call_params, timeout=llm_timeout)

            # Debug: Log full response details
            logger.info(f"[Agent] OpenAI raw response: id={response.id}, model={response.model}, choices={len(response.choices)}")
            if response.choices:
                choice = response.choices[0]
                logger.info(f"[Agent] Choice[0]: finish_reason={choice.finish_reason}, content_len={len(choice.message.content) if choice.message.content else 0}, tool_calls={len(choice.message.tool_calls) if choice.message.tool_calls else 0}")

                # Check for refusal (o4-mini specific)
                if hasattr(choice.message, 'refusal') and choice.message.refusal:
                    logger.warning(f"[Agent] Model refused: {choice.message.refusal}")
                    return {"content": f"Model refused: {choice.message.refusal}", "tool_calls": []}
            else:
                logger.error(f"[Agent] No choices in response!")
                return {"content": "Error: No response from model", "tool_calls": []}

            # Parse response
            result = {"content": "", "tool_calls": []}

            if choice.message.content:
                result["content"] = choice.message.content

            if choice.message.tool_calls:
                for tc in choice.message.tool_calls:
                    result["tool_calls"].append({
                        "id": tc.id,
                        "type": "function",
                        "function": {
                            "name": tc.function.name,
                            "arguments": tc.function.arguments
                        }
                    })

            logger.info(f"[Agent] OpenAI response: content={len(result['content'])} chars, tool_calls={len(result['tool_calls'])}")
            return result

        except Exception as e:
            error_type = type(e).__name__
            # Check for timeout errors (openai.APITimeoutError or similar)
            if "Timeout" in error_type or "timeout" in str(e).lower():
                llm_timeout = self._get_llm_timeout()
                logger.error(f"[Agent] OpenAI API timeout after {llm_timeout}s: {e}")
                return {"content": f"Error: LLM API timeout ({llm_timeout}ç§’). ã‚·ã‚¹ãƒ†ãƒ è¨­å®šã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå€¤ã‚’å¢—ã‚„ã™ã‹ã€å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚", "tool_calls": []}
            logger.error(f"[Agent] OpenAI tool call error: {e}", exc_info=True)
            return {"content": f"Error: {str(e)}", "tool_calls": []}

    def run_sync(self, session: AgentSession, user_message: str) -> str:
        """Synchronous wrapper for run()."""
        import asyncio
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(self.run(session, user_message))
        finally:
            loop.close()


# Convenience functions
def create_agent(model_name: str = None, temperature: float = 0.7) -> Tuple[AgentEngine, AgentSession]:
    """Create an agent engine and session.

    Returns:
        Tuple of (engine, session)
    """
    engine = AgentEngine(model_name=model_name, temperature=temperature)
    session = engine.create_session()
    return engine, session


async def run_agent(instruction: str, model_name: str = None,
                    temperature: float = 0.7) -> str:
    """Run an agent with a single instruction.

    Args:
        instruction: The task for the agent
        model_name: LLM model to use
        temperature: Temperature for LLM

    Returns:
        The agent's response
    """
    engine, session = create_agent(model_name, temperature)
    return await engine.run(session, instruction)
